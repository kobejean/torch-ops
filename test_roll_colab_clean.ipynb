{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom CUDA Roll Operation\n",
    "\n",
    "Testing custom torch.roll implementation with CUDA optimization.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kobejean/torch-ops/blob/main/test_roll_colab_clean.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/kobejean/torch-ops.git\n",
    "%cd torch-ops\n",
    "!pip install -e . -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import custom_ops\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correctness test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic functionality\n",
    "x = torch.randn(100, 100, device='cuda')\n",
    "shifts, dims = [10, 20], [0, 1]\n",
    "\n",
    "result_custom = custom_ops.roll(x, shifts, dims)\n",
    "result_pytorch = torch.roll(x, shifts, dims)\n",
    "\n",
    "print(f\"Results match: {torch.allclose(result_custom, result_pytorch)}\")\n",
    "print(f\"Max difference: {torch.max(torch.abs(result_custom - result_pytorch)).item():.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [(500, 500), (1000, 1000), (2000, 2000)]\n",
    "\n",
    "print(f\"{'Size':<12} {'Custom (ms)':<12} {'PyTorch (ms)':<13} {'Speedup':<8}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for size in sizes:\n",
    "    x = torch.randn(size, device='cuda')\n",
    "    shifts = [size[0]//10, size[1]//10]\n",
    "    dims = [0, 1]\n",
    "    \n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        custom_ops.roll(x, shifts, dims)\n",
    "        torch.roll(x, shifts, dims)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Time custom implementation\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    \n",
    "    start.record()\n",
    "    for _ in range(20):\n",
    "        custom_ops.roll(x, shifts, dims)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    custom_time = start.elapsed_time(end) / 20\n",
    "    \n",
    "    # Time PyTorch implementation\n",
    "    start.record()\n",
    "    for _ in range(20):\n",
    "        torch.roll(x, shifts, dims)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    pytorch_time = start.elapsed_time(end) / 20\n",
    "    \n",
    "    speedup = pytorch_time / custom_time\n",
    "    print(f\"{str(size):<12} {custom_time:<12.3f} {pytorch_time:<13.3f} {speedup:<8.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get GPU properties\n",
    "props = torch.cuda.get_device_properties(0)\n",
    "print(f\"GPU: {props.name}\")\n",
    "print(f\"Compute Capability: {props.major}.{props.minor}\")\n",
    "print(f\"Multiprocessors: {props.multi_processor_count}\")\n",
    "print(f\"Max threads per MP: {props.max_threads_per_multi_processor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate occupancy for our kernel\n",
    "threads_per_block = 256\n",
    "warps_per_block = threads_per_block // 32\n",
    "max_blocks = props.max_threads_per_multi_processor // threads_per_block\n",
    "max_warps = props.max_threads_per_multi_processor // 32\n",
    "occupancy = (max_blocks * warps_per_block) / max_warps * 100\n",
    "\n",
    "print(f\"Threads per block: {threads_per_block}\")\n",
    "print(f\"Warps per block: {warps_per_block}\")\n",
    "print(f\"Max active blocks: {max_blocks}\")\n",
    "print(f\"Theoretical occupancy: {occupancy:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory bandwidth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure memory bandwidth\n",
    "x = torch.randn(1000, 1000, device='cuda')\n",
    "bytes_transferred = x.numel() * 4 * 2  # read + write float32\n",
    "\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "start.record()\n",
    "for _ in range(10):\n",
    "    custom_ops.roll(x, [100, 200], [0, 1])\n",
    "end.record()\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "time_ms = start.elapsed_time(end) / 10\n",
    "bandwidth = (bytes_transferred / 1e9) / (time_ms / 1000)\n",
    "print(f\"Kernel time: {time_ms:.3f} ms\")\n",
    "print(f\"Memory bandwidth: {bandwidth:.1f} GB/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiler output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.profiler\n",
    "\n",
    "x = torch.randn(1000, 1000, device='cuda')\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    activities=[torch.profiler.ProfilerActivity.CUDA],\n",
    "    record_shapes=True\n",
    ") as prof:\n",
    "    for _ in range(5):\n",
    "        custom_ops.roll(x, [100, 200], [0, 1])\n",
    "\n",
    "# Show kernel details\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}