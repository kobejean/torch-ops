{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkI3IZH5VTTQ"
      },
      "source": [
        "# Custom CUDA Roll Operation\n",
        "\n",
        "Testing custom torch.roll implementation with CUDA optimization.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kobejean/torch-ops/blob/main/test_roll_colab.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrJvieHxVTTR"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iQm_H6pIVTTR",
        "outputId": "1275bb25-7a63-4eae-fc60-c0c5a943fade",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch: 2.8.0+cu126\n",
            "CUDA: True\n",
            "Device: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Device: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "erHqVq5vVTTR",
        "outputId": "759990d0-5d36-4298-db5d-db3748691e3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'torch-ops'...\n",
            "remote: Enumerating objects: 77, done.\u001b[K\n",
            "remote: Counting objects: 100% (77/77), done.\u001b[K\n",
            "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
            "remote: Total 77 (delta 33), reused 66 (delta 22), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (77/77), 33.67 KiB | 4.21 MiB/s, done.\n",
            "Resolving deltas: 100% (33/33), done.\n",
            "/content/torch-ops\n",
            "Obtaining file:///content/torch-ops\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Installing collected packages: custom_ops\n",
            "  Running setup.py develop for custom_ops\n",
            "Successfully installed custom_ops-0.0.0\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/kobejean/torch-ops.git\n",
        "%cd torch-ops\n",
        "!pip install -e . -q -v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ISbY1-vDVTTS"
      },
      "outputs": [],
      "source": [
        "import custom_ops\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybAcaOthVTTS"
      },
      "source": [
        "## Correctness test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EUtzhvWfVTTS",
        "outputId": "2f9c8055-3736-4a60-dab6-33e0fe6c098c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results match: True\n",
            "Max difference: 0.00e+00\n"
          ]
        }
      ],
      "source": [
        "# Test basic functionality\n",
        "x = torch.randn(100, 100, device='cuda')\n",
        "shifts, dims = [10, 20], [0, 1]\n",
        "\n",
        "result_custom = custom_ops.roll(x, shifts, dims)\n",
        "result_pytorch = torch.roll(x, shifts, dims)\n",
        "\n",
        "print(f\"Results match: {torch.allclose(result_custom, result_pytorch)}\")\n",
        "print(f\"Max difference: {torch.max(torch.abs(result_custom - result_pytorch)).item():.2e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lq6cAjptVTTS"
      },
      "source": [
        "## Performance comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pyUI9s-0VTTS",
        "outputId": "2452e875-7bd1-450a-a8fd-7107cb3c5bf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size         Custom (ms)  PyTorch (ms)  Speedup \n",
            "--------------------------------------------------\n",
            "(500, 500)   0.064        0.037         0.57    x\n",
            "(1000, 1000) 0.139        0.132         0.95    x\n",
            "(5000, 5000) 2.588        3.021         1.17    x\n",
            "(10000, 10000) 5.521        7.123         1.29    x\n"
          ]
        }
      ],
      "source": [
        "sizes = [(500, 500), (1000, 1000), (5000, 5000), (10000, 10000)]\n",
        "\n",
        "print(f\"{'Size':<12} {'Custom (ms)':<12} {'PyTorch (ms)':<13} {'Speedup':<8}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for size in sizes:\n",
        "    x = torch.randn(size, device='cuda')\n",
        "    shifts = [size[0]//10, size[1]//10]\n",
        "    dims = [0, 1]\n",
        "\n",
        "    # Warmup\n",
        "    for _ in range(5):\n",
        "        custom_ops.roll(x, shifts, dims)\n",
        "        torch.roll(x, shifts, dims)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    # Time custom implementation\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "    start.record()\n",
        "    for _ in range(20):\n",
        "        custom_ops.roll(x, shifts, dims)\n",
        "    end.record()\n",
        "    torch.cuda.synchronize()\n",
        "    custom_time = start.elapsed_time(end) / 20\n",
        "\n",
        "    # Time PyTorch implementation\n",
        "    start.record()\n",
        "    for _ in range(20):\n",
        "        torch.roll(x, shifts, dims)\n",
        "    end.record()\n",
        "    torch.cuda.synchronize()\n",
        "    pytorch_time = start.elapsed_time(end) / 20\n",
        "\n",
        "    speedup = pytorch_time / custom_time\n",
        "    print(f\"{str(size):<12} {custom_time:<12.3f} {pytorch_time:<13.3f} {speedup:<8.2f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnoCD0NFVTTS"
      },
      "source": [
        "## Kernel analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lwJV_hsQVTTS",
        "outputId": "3fad37c9-ac64-4944-e7e3-e5115d95fcd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: Tesla T4\n",
            "Compute Capability: 7.5\n",
            "Multiprocessors: 40\n",
            "Max threads per MP: 1024\n"
          ]
        }
      ],
      "source": [
        "# Get GPU properties\n",
        "props = torch.cuda.get_device_properties(0)\n",
        "print(f\"GPU: {props.name}\")\n",
        "print(f\"Compute Capability: {props.major}.{props.minor}\")\n",
        "print(f\"Multiprocessors: {props.multi_processor_count}\")\n",
        "print(f\"Max threads per MP: {props.max_threads_per_multi_processor}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jK5NibdLVTTS",
        "outputId": "86746f3d-8472-4d4e-8fb5-b0a059edb6ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Threads per block: 256\n",
            "Warps per block: 8\n",
            "Max active blocks: 4\n",
            "Theoretical occupancy: 100.0%\n"
          ]
        }
      ],
      "source": [
        "# Calculate occupancy for our kernel\n",
        "threads_per_block = 256\n",
        "warps_per_block = threads_per_block // 32\n",
        "max_blocks = props.max_threads_per_multi_processor // threads_per_block\n",
        "max_warps = props.max_threads_per_multi_processor // 32\n",
        "occupancy = (max_blocks * warps_per_block) / max_warps * 100\n",
        "\n",
        "print(f\"Threads per block: {threads_per_block}\")\n",
        "print(f\"Warps per block: {warps_per_block}\")\n",
        "print(f\"Max active blocks: {max_blocks}\")\n",
        "print(f\"Theoretical occupancy: {occupancy:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyhmOq76VTTS"
      },
      "source": [
        "## Memory bandwidth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZMIJ8XZ3VTTS",
        "outputId": "ad2ff302-8fab-4874-c197-f50688fd3a32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kernel time: 0.194 ms\n",
            "Memory bandwidth: 41.2 GB/s\n"
          ]
        }
      ],
      "source": [
        "# Measure memory bandwidth\n",
        "x = torch.randn(1000, 1000, device='cuda')\n",
        "bytes_transferred = x.numel() * 4 * 2  # read + write float32\n",
        "\n",
        "start = torch.cuda.Event(enable_timing=True)\n",
        "end = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "start.record()\n",
        "for _ in range(10):\n",
        "    custom_ops.roll(x, [100, 200], [0, 1])\n",
        "end.record()\n",
        "torch.cuda.synchronize()\n",
        "\n",
        "time_ms = start.elapsed_time(end) / 10\n",
        "bandwidth = (bytes_transferred / 1e9) / (time_ms / 1000)\n",
        "print(f\"Kernel time: {time_ms:.3f} ms\")\n",
        "print(f\"Memory bandwidth: {bandwidth:.1f} GB/s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIRoduMiVTTS"
      },
      "source": [
        "## Profiler output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oKhA8t6QVTTT",
        "outputId": "dbcc1968-e94a-4f62-f3ff-a5dfd4b9f1b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "void custom_ops::tensor::roll_kernel<float>(float co...         0.00%       0.000us         0.00%       0.000us       0.000us     543.070us        98.18%     543.070us     108.614us             5  \n",
            "                       Memcpy HtoD (Pageable -> Device)         0.00%       0.000us         0.00%       0.000us       0.000us      10.047us         1.82%      10.047us       0.670us            15  \n",
            "                                     cudaMemcpyToSymbol        32.42%       1.319ms        97.99%       3.986ms     265.727us       0.000us         0.00%       0.000us       0.000us            15  \n",
            "                                           Unrecognized        65.57%       2.667ms        65.57%       2.667ms       2.667ms       0.000us         0.00%       0.000us       0.000us             1  \n",
            "                                       cudaLaunchKernel         1.81%      73.605us         1.81%      73.605us      14.721us       0.000us         0.00%       0.000us       0.000us             5  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 4.068ms\n",
            "Self CUDA time total: 553.117us\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch.profiler\n",
        "\n",
        "x = torch.randn(1000, 1000, device='cuda')\n",
        "\n",
        "with torch.profiler.profile(\n",
        "    activities=[torch.profiler.ProfilerActivity.CUDA],\n",
        "    record_shapes=True\n",
        ") as prof:\n",
        "    for _ in range(5):\n",
        "        custom_ops.roll(x, [100, 200], [0, 1])\n",
        "\n",
        "# Show kernel details\n",
        "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=5))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}