{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kobejean/torch-ops/blob/main/test_roll_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom PyTorch Roll Operation Test\n",
    "\n",
    "This notebook tests the custom `torch.roll` implementation with both CPU and CUDA support.\n",
    "The implementation includes optimized CUDA kernels inspired by TensorFlow's roll operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we're in Colab and have GPU access\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone Repository and Build Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/kobejean/torch-ops.git\n",
    "%cd torch-ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the extension\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Basic Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import torch\nimport custom_ops  # Import the custom_ops module directly\nimport numpy as np\nimport time\n\nprint(\"Custom ops available:\")\nprint(dir(custom_ops))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test CPU Roll Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def test_cpu_roll():\n    print(\"=== CPU Roll Tests ===\")\n    \n    # Test 1: Basic 2D roll\n    x = torch.arange(12, dtype=torch.float32).reshape(3, 4)\n    print(f\"Original tensor:\\n{x}\")\n    \n    # Roll along dimension 0\n    result = custom_ops.roll(x, [1], [0])\n    expected = torch.roll(x, [1], [0])\n    print(f\"\\nRoll by 1 along dim 0:\")\n    print(f\"Custom: \\n{result}\")\n    print(f\"PyTorch:\\n{expected}\")\n    print(f\"Match: {torch.allclose(result, expected)}\")\n    \n    # Test 2: Multi-dimensional roll\n    result = custom_ops.roll(x, [1, 2], [0, 1])\n    expected = torch.roll(x, [1, 2], [0, 1])\n    print(f\"\\nRoll by [1, 2] along dims [0, 1]:\")\n    print(f\"Custom: \\n{result}\")\n    print(f\"PyTorch:\\n{expected}\")\n    print(f\"Match: {torch.allclose(result, expected)}\")\n    \n    # Test 3: Negative shifts\n    result = custom_ops.roll(x, [-1, -2], [0, 1])\n    expected = torch.roll(x, [-1, -2], [0, 1])\n    print(f\"\\nRoll by [-1, -2] along dims [0, 1]:\")\n    print(f\"Custom: \\n{result}\")\n    print(f\"PyTorch:\\n{expected}\")\n    print(f\"Match: {torch.allclose(result, expected)}\")\n    \n    # Test 4: Zero shifts (should be no-op)\n    result = custom_ops.roll(x, [0, 0], [0, 1])\n    print(f\"\\nZero shift test:\")\n    print(f\"Match original: {torch.allclose(result, x)}\")\n    \n    return True\n\ntest_cpu_roll()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test CUDA Roll Operation (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def test_cuda_roll():\n    if not torch.cuda.is_available():\n        print(\"CUDA not available, skipping CUDA tests\")\n        return False\n        \n    print(\"=== CUDA Roll Tests ===\")\n    \n    # Test 1: Basic CUDA roll\n    x_cpu = torch.arange(12, dtype=torch.float32).reshape(3, 4)\n    x_cuda = x_cpu.cuda()\n    \n    print(f\"Original tensor:\\n{x_cpu}\")\n    \n    # Test single dimension roll\n    result_cuda = custom_ops.roll(x_cuda, [1], [0])\n    expected_cuda = torch.roll(x_cuda, [1], [0])\n    \n    print(f\"\\nCUDA Roll by 1 along dim 0:\")\n    print(f\"Custom: \\n{result_cuda.cpu()}\")\n    print(f\"PyTorch:\\n{expected_cuda.cpu()}\")\n    print(f\"Match: {torch.allclose(result_cuda, expected_cuda)}\")\n    \n    # Test 2: Multi-dimensional CUDA roll\n    result_cuda = custom_ops.roll(x_cuda, [1, 2], [0, 1])\n    expected_cuda = torch.roll(x_cuda, [1, 2], [0, 1])\n    \n    print(f\"\\nCUDA Roll by [1, 2] along dims [0, 1]:\")\n    print(f\"Custom: \\n{result_cuda.cpu()}\")\n    print(f\"PyTorch:\\n{expected_cuda.cpu()}\")\n    print(f\"Match: {torch.allclose(result_cuda, expected_cuda)}\")\n    \n    # Test 3: Larger tensor for performance\n    large_tensor = torch.randn(100, 100, device='cuda')\n    result_large = custom_ops.roll(large_tensor, [10, 20], [0, 1])\n    expected_large = torch.roll(large_tensor, [10, 20], [0, 1])\n    \n    print(f\"\\nLarge tensor (100x100) test:\")\n    print(f\"Match: {torch.allclose(result_large, expected_large)}\")\n    print(f\"Max difference: {torch.max(torch.abs(result_large - expected_large)).item()}\")\n    \n    return True\n\ntest_cuda_roll()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def benchmark_roll():\n    print(\"=== Performance Benchmark ===\")\n    \n    # CPU Benchmark\n    print(\"\\nCPU Benchmark:\")\n    sizes = [(100, 100), (500, 500), (1000, 1000)]\n    \n    for size in sizes:\n        x = torch.randn(size, dtype=torch.float32)\n        shifts = [10, 20]\n        dims = [0, 1]\n        \n        # Warmup\n        for _ in range(5):\n            _ = custom_ops.roll(x, shifts, dims)\n            _ = torch.roll(x, shifts, dims)\n        \n        # Benchmark custom\n        start = time.time()\n        for _ in range(50):\n            result_custom = custom_ops.roll(x, shifts, dims)\n        custom_time = time.time() - start\n        \n        # Benchmark PyTorch\n        start = time.time()\n        for _ in range(50):\n            result_pytorch = torch.roll(x, shifts, dims)\n        pytorch_time = time.time() - start\n        \n        print(f\"Size {size}: Custom={custom_time:.4f}s, PyTorch={pytorch_time:.4f}s, Ratio={pytorch_time/custom_time:.2f}x\")\n        \n        # Verify correctness\n        assert torch.allclose(result_custom, result_pytorch), f\"Mismatch for size {size}\"\n    \n    # CUDA Benchmark\n    if torch.cuda.is_available():\n        print(\"\\nCUDA Benchmark:\")\n        \n        for size in sizes:\n            x = torch.randn(size, dtype=torch.float32, device='cuda')\n            shifts = [10, 20]\n            dims = [0, 1]\n            \n            # Warmup\n            for _ in range(10):\n                _ = custom_ops.roll(x, shifts, dims)\n                _ = torch.roll(x, shifts, dims)\n            torch.cuda.synchronize()\n            \n            # Benchmark custom\n            start = time.time()\n            for _ in range(100):\n                result_custom = custom_ops.roll(x, shifts, dims)\n            torch.cuda.synchronize()\n            custom_time = time.time() - start\n            \n            # Benchmark PyTorch\n            start = time.time()\n            for _ in range(100):\n                result_pytorch = torch.roll(x, shifts, dims)\n            torch.cuda.synchronize()\n            pytorch_time = time.time() - start\n            \n            print(f\"Size {size}: Custom={custom_time:.4f}s, PyTorch={pytorch_time:.4f}s, Ratio={pytorch_time/custom_time:.2f}x\")\n            \n            # Verify correctness\n            assert torch.allclose(result_custom, result_pytorch), f\"CUDA mismatch for size {size}\"\n\nbenchmark_roll()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge Case Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def test_edge_cases():\n    print(\"=== Edge Case Tests ===\")\n    \n    # Test 1: 1D tensor\n    x1d = torch.arange(5, dtype=torch.float32)\n    result = custom_ops.roll(x1d, [2], [0])\n    expected = torch.roll(x1d, [2], [0])\n    print(f\"1D tensor test: {torch.allclose(result, expected)}\")\n    \n    # Test 2: 3D tensor\n    x3d = torch.arange(24, dtype=torch.float32).reshape(2, 3, 4)\n    result = custom_ops.roll(x3d, [1, 1, 2], [0, 1, 2])\n    expected = torch.roll(x3d, [1, 1, 2], [0, 1, 2])\n    print(f\"3D tensor test: {torch.allclose(result, expected)}\")\n    \n    # Test 3: Empty tensor\n    empty = torch.empty((0, 5), dtype=torch.float32)\n    result = custom_ops.roll(empty, [1], [1])\n    expected = torch.roll(empty, [1], [1])\n    print(f\"Empty tensor test: {result.shape == expected.shape and torch.allclose(result, expected)}\")\n    \n    # Test 4: Large shifts (wrapping)\n    x = torch.arange(12, dtype=torch.float32).reshape(3, 4)\n    result = custom_ops.roll(x, [10], [0])  # 10 > 3, should wrap\n    expected = torch.roll(x, [10], [0])\n    print(f\"Large shift test: {torch.allclose(result, expected)}\")\n    \n    # Test 5: Negative dimension indices\n    result = custom_ops.roll(x, [1], [-1])  # -1 should be dimension 1\n    expected = torch.roll(x, [1], [-1])\n    print(f\"Negative dim test: {torch.allclose(result, expected)}\")\n    \n    # Test 6: Different dtypes\n    for dtype in [torch.int32, torch.int64, torch.float64]:\n        x_dtype = x.to(dtype)\n        result = custom_ops.roll(x_dtype, [1], [0])\n        expected = torch.roll(x_dtype, [1], [0])\n        print(f\"{dtype} test: {torch.allclose(result, expected)}\")\n    \n    print(\"All edge case tests completed!\")\n\ntest_edge_cases()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🎉 All tests completed successfully!\")\n",
    "print(\"\\nCustom torch.roll implementation features:\")\n",
    "print(\"✅ CPU and CUDA support\")\n",
    "print(\"✅ Multi-dimensional rolling\")\n",
    "print(\"✅ Negative shifts and dimensions\")\n",
    "print(\"✅ Multiple data types (float32, float64, int32, int64)\")\n",
    "print(\"✅ Optimized CUDA kernel with branch-free execution\")\n",
    "print(\"✅ Edge case handling (empty tensors, large shifts, etc.)\")\n",
    "print(\"✅ Compatible with PyTorch's torch.roll interface\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4",
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}