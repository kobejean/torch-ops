{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/kobejean/torch-ops/blob/main/test_roll_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom PyTorch Roll Operation Test\n",
    "\n",
    "This notebook tests the custom `torch.roll` implementation with both CPU and CUDA support.\n",
    "The implementation includes optimized CUDA kernels inspired by TensorFlow's roll operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we're in Colab and have GPU access\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clone Repository and Build Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/kobejean/torch-ops.git\n",
    "%cd torch-ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the extension\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Basic Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import custom_ops  # Import the custom_ops module directly\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print(\"Custom ops available:\")\n",
    "print(dir(custom_ops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test CPU Roll Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cpu_roll():\n",
    "    print(\"=== CPU Roll Tests ===\")\n",
    "    \n",
    "    # Test 1: Basic 2D roll\n",
    "    x = torch.arange(12, dtype=torch.float32).reshape(3, 4)\n",
    "    print(f\"Original tensor:\\n{x}\")\n",
    "    \n",
    "    # Roll along dimension 0\n",
    "    result = custom_ops.roll(x, [1], [0])\n",
    "    expected = torch.roll(x, [1], [0])\n",
    "    print(f\"\\nRoll by 1 along dim 0:\")\n",
    "    print(f\"Custom: \\n{result}\")\n",
    "    print(f\"PyTorch:\\n{expected}\")\n",
    "    print(f\"Match: {torch.allclose(result, expected)}\")\n",
    "    \n",
    "    # Test 2: Multi-dimensional roll\n",
    "    result = custom_ops.roll(x, [1, 2], [0, 1])\n",
    "    expected = torch.roll(x, [1, 2], [0, 1])\n",
    "    print(f\"\\nRoll by [1, 2] along dims [0, 1]:\")\n",
    "    print(f\"Custom: \\n{result}\")\n",
    "    print(f\"PyTorch:\\n{expected}\")\n",
    "    print(f\"Match: {torch.allclose(result, expected)}\")\n",
    "    \n",
    "    # Test 3: Negative shifts\n",
    "    result = custom_ops.roll(x, [-1, -2], [0, 1])\n",
    "    expected = torch.roll(x, [-1, -2], [0, 1])\n",
    "    print(f\"\\nRoll by [-1, -2] along dims [0, 1]:\")\n",
    "    print(f\"Custom: \\n{result}\")\n",
    "    print(f\"PyTorch:\\n{expected}\")\n",
    "    print(f\"Match: {torch.allclose(result, expected)}\")\n",
    "    \n",
    "    # Test 4: Zero shifts (should be no-op)\n",
    "    result = custom_ops.roll(x, [0, 0], [0, 1])\n",
    "    print(f\"\\nZero shift test:\")\n",
    "    print(f\"Match original: {torch.allclose(result, x)}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "test_cpu_roll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test CUDA Roll Operation (if available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cuda_roll():\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA not available, skipping CUDA tests\")\n",
    "        return False\n",
    "        \n",
    "    print(\"=== CUDA Roll Tests ===\")\n",
    "    \n",
    "    # Test 1: Basic CUDA roll\n",
    "    x_cpu = torch.arange(12, dtype=torch.float32).reshape(3, 4)\n",
    "    x_cuda = x_cpu.cuda()\n",
    "    \n",
    "    print(f\"Original tensor:\\n{x_cpu}\")\n",
    "    \n",
    "    # Test single dimension roll\n",
    "    result_cuda = custom_ops.roll(x_cuda, [1], [0])\n",
    "    expected_cuda = torch.roll(x_cuda, [1], [0])\n",
    "    \n",
    "    print(f\"\\nCUDA Roll by 1 along dim 0:\")\n",
    "    print(f\"Custom: \\n{result_cuda.cpu()}\")\n",
    "    print(f\"PyTorch:\\n{expected_cuda.cpu()}\")\n",
    "    print(f\"Match: {torch.allclose(result_cuda, expected_cuda)}\")\n",
    "    \n",
    "    # Test 2: Multi-dimensional CUDA roll\n",
    "    result_cuda = custom_ops.roll(x_cuda, [1, 2], [0, 1])\n",
    "    expected_cuda = torch.roll(x_cuda, [1, 2], [0, 1])\n",
    "    \n",
    "    print(f\"\\nCUDA Roll by [1, 2] along dims [0, 1]:\")\n",
    "    print(f\"Custom: \\n{result_cuda.cpu()}\")\n",
    "    print(f\"PyTorch:\\n{expected_cuda.cpu()}\")\n",
    "    print(f\"Match: {torch.allclose(result_cuda, expected_cuda)}\")\n",
    "    \n",
    "    # Test 3: Larger tensor for performance\n",
    "    large_tensor = torch.randn(100, 100, device='cuda')\n",
    "    result_large = custom_ops.roll(large_tensor, [10, 20], [0, 1])\n",
    "    expected_large = torch.roll(large_tensor, [10, 20], [0, 1])\n",
    "    \n",
    "    print(f\"\\nLarge tensor (100x100) test:\")\n",
    "    print(f\"Match: {torch.allclose(result_large, expected_large)}\")\n",
    "    print(f\"Max difference: {torch.max(torch.abs(result_large - expected_large)).item()}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "test_cuda_roll()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def benchmark_roll():\n    print(\"=== Performance Benchmark ===\")\n    \n    # Test configurations: (size, shifts, dims, description)\n    test_configs = [\n        # 2D benchmarks with different shift patterns\n        ((100, 100), [10, 20], [0, 1], \"2D multi-dim shifts\"),\n        ((500, 500), [50, 100], [0, 1], \"2D multi-dim shifts\"),\n        ((1000, 1000), [100, 200], [0, 1], \"2D multi-dim shifts\"),\n        \n        # Single dimension shifts for comparison\n        ((100, 100), [10], [0], \"2D single-dim shift\"),\n        ((500, 500), [50], [0], \"2D single-dim shift\"),\n        ((1000, 1000), [100], [0], \"2D single-dim shift\"),\n        \n        # 3D benchmarks\n        ((50, 50, 50), [5, 10, 15], [0, 1, 2], \"3D multi-dim shifts\"),\n        ((100, 100, 100), [10, 20, 30], [0, 1, 2], \"3D multi-dim shifts\"),\n        \n        # Large 2D tensors for scaling test\n        ((5000, 5000), [500, 1000], [0, 1], \"Large 2D multi-dim\"),\n    ]\n    \n    for size, shifts, dims, description in test_configs:\n        print(f\"\\n--- {description}: {size} ---\")\n        \n        # CPU Benchmark\n        x_cpu = torch.randn(size, dtype=torch.float32)\n        \n        # Warmup\n        for _ in range(3):\n            _ = custom_ops.roll(x_cpu, shifts, dims)\n            _ = torch.roll(x_cpu, shifts, dims)\n        \n        # Benchmark custom CPU\n        num_iterations = 20 if max(size) < 1000 else 10\n        start = time.time()\n        for _ in range(num_iterations):\n            result_custom = custom_ops.roll(x_cpu, shifts, dims)\n        custom_time = time.time() - start\n        \n        # Benchmark PyTorch CPU\n        start = time.time()\n        for _ in range(num_iterations):\n            result_pytorch = torch.roll(x_cpu, shifts, dims)\n        pytorch_time = time.time() - start\n        \n        print(f\"CPU: Custom={custom_time:.4f}s, PyTorch={pytorch_time:.4f}s, Ratio={pytorch_time/custom_time:.2f}x\")\n        \n        # Verify correctness\n        assert torch.allclose(result_custom, result_pytorch), f\"CPU mismatch for {description}\"\n        \n        # CUDA Benchmark (if available)\n        if torch.cuda.is_available():\n            x_cuda = x_cpu.cuda()\n            \n            # Warmup\n            for _ in range(5):\n                _ = custom_ops.roll(x_cuda, shifts, dims)\n                _ = torch.roll(x_cuda, shifts, dims)\n            torch.cuda.synchronize()\n            \n            # Benchmark custom CUDA\n            num_iterations = 50 if max(size) < 1000 else 20\n            start = time.time()\n            for _ in range(num_iterations):\n                result_custom = custom_ops.roll(x_cuda, shifts, dims)\n            torch.cuda.synchronize()\n            custom_time = time.time() - start\n            \n            # Benchmark PyTorch CUDA\n            start = time.time()\n            for _ in range(num_iterations):\n                result_pytorch = torch.roll(x_cuda, shifts, dims)\n            torch.cuda.synchronize()\n            pytorch_time = time.time() - start\n            \n            print(f\"CUDA: Custom={custom_time:.4f}s, PyTorch={pytorch_time:.4f}s, Ratio={pytorch_time/custom_time:.2f}x\")\n            \n            # Verify correctness\n            assert torch.allclose(result_custom, result_pytorch), f\"CUDA mismatch for {description}\"\n    \n    print(\"\\n=== Summary ===\")\n    print(\"✅ All correctness tests passed!\")\n    print(\"📊 Benchmarks completed for various tensor sizes and shift patterns\")\n    print(\"🔧 Optimized implementation uses:\")\n    print(\"   - Constant memory for dimension data\")  \n    print(\"   - Shared memory for fast block-local access\")\n    print(\"   - Branch-free kernel execution\")\n\nbenchmark_roll()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge Case Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_edge_cases():\n",
    "    print(\"=== Edge Case Tests ===\")\n",
    "    \n",
    "    # Test 1: 1D tensor\n",
    "    x1d = torch.arange(5, dtype=torch.float32)\n",
    "    result = custom_ops.roll(x1d, [2], [0])\n",
    "    expected = torch.roll(x1d, [2], [0])\n",
    "    print(f\"1D tensor test: {torch.allclose(result, expected)}\")\n",
    "    \n",
    "    # Test 2: 3D tensor\n",
    "    x3d = torch.arange(24, dtype=torch.float32).reshape(2, 3, 4)\n",
    "    result = custom_ops.roll(x3d, [1, 1, 2], [0, 1, 2])\n",
    "    expected = torch.roll(x3d, [1, 1, 2], [0, 1, 2])\n",
    "    print(f\"3D tensor test: {torch.allclose(result, expected)}\")\n",
    "    \n",
    "    # Test 3: Empty tensor\n",
    "    empty = torch.empty((0, 5), dtype=torch.float32)\n",
    "    result = custom_ops.roll(empty, [1], [1])\n",
    "    expected = torch.roll(empty, [1], [1])\n",
    "    print(f\"Empty tensor test: {result.shape == expected.shape and torch.allclose(result, expected)}\")\n",
    "    \n",
    "    # Test 4: Large shifts (wrapping)\n",
    "    x = torch.arange(12, dtype=torch.float32).reshape(3, 4)\n",
    "    result = custom_ops.roll(x, [10], [0])  # 10 > 3, should wrap\n",
    "    expected = torch.roll(x, [10], [0])\n",
    "    print(f\"Large shift test: {torch.allclose(result, expected)}\")\n",
    "    \n",
    "    # Test 5: Negative dimension indices\n",
    "    result = custom_ops.roll(x, [1], [-1])  # -1 should be dimension 1\n",
    "    expected = torch.roll(x, [1], [-1])\n",
    "    print(f\"Negative dim test: {torch.allclose(result, expected)}\")\n",
    "    \n",
    "    # Test 6: Different dtypes\n",
    "    for dtype in [torch.int32, torch.int64, torch.float64]:\n",
    "        x_dtype = x.to(dtype)\n",
    "        result = custom_ops.roll(x_dtype, [1], [0])\n",
    "        expected = torch.roll(x_dtype, [1], [0])\n",
    "        print(f\"{dtype} test: {torch.allclose(result, expected)}\")\n",
    "    \n",
    "    print(\"All edge case tests completed!\")\n",
    "\n",
    "test_edge_cases()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## CUDA Profiling and Analysis"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check available profiling tools in Colab\nimport subprocess\nimport os\n\nprint(\"=== CUDA Profiling Setup ===\")\n\n# Check NVIDIA tools availability\ntools_to_check = [\n    ('nvprof', 'NVIDIA profiler'),\n    ('ncu', 'Nsight Compute'),\n    ('nvidia-smi', 'NVIDIA System Management'),\n]\n\navailable_tools = []\nfor tool, description in tools_to_check:\n    try:\n        result = subprocess.run([tool, '--version'], capture_output=True, text=True, timeout=5)\n        if result.returncode == 0:\n            print(f\"✅ {description} ({tool}): Available\")\n            available_tools.append(tool)\n        else:\n            print(f\"❌ {description} ({tool}): Not available\")\n    except:\n        print(f\"❌ {description} ({tool}): Not found\")\n\n# Check GPU info\ntry:\n    result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)\n    if result.returncode == 0:\n        print(f\"\\n=== GPU Information ===\")\n        lines = result.stdout.split('\\n')\n        for line in lines:\n            if 'Tesla' in line or 'Quadro' in line or 'GeForce' in line or 'GPU' in line:\n                print(line.strip())\n    else:\n        print(\"❌ Could not get GPU information\")\nexcept:\n    print(\"❌ nvidia-smi not available\")\n\nprint(f\"\\n=== Available Profiling Methods ===\")\nif available_tools:\n    print(\"We can use the following profiling approaches:\")\n    if 'nvprof' in available_tools:\n        print(\"1. nvprof - Legacy profiler (deprecated but often available)\")\n    if 'ncu' in available_tools:  \n        print(\"2. ncu (Nsight Compute) - Modern profiler\")\n    print(\"3. PyTorch Profiler - Built into PyTorch\")\n    print(\"4. CUDA Events - Manual timing within code\")\nelse:\n    print(\"Limited profiling tools available, will use PyTorch profiler and CUDA events\")"
  },
  {
   "cell_type": "code",
   "source": "# PyTorch Profiler Analysis\nimport torch.profiler\n\ndef profile_roll_operations():\n    \"\"\"Profile our custom roll vs PyTorch roll using PyTorch's built-in profiler\"\"\"\n    \n    if not torch.cuda.is_available():\n        print(\"CUDA not available for profiling\")\n        return\n        \n    print(\"=== PyTorch Profiler Analysis ===\")\n    \n    # Test tensor\n    x = torch.randn(1000, 1000, device='cuda', dtype=torch.float32)\n    shifts = [100, 200]  \n    dims = [0, 1]\n    \n    # Profile custom implementation\n    print(\"\\n--- Profiling Custom Roll ---\")\n    with torch.profiler.profile(\n        activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n        record_shapes=True,\n        profile_memory=True,\n        with_stack=True\n    ) as prof:\n        with torch.profiler.record_function(\"custom_roll\"):\n            for _ in range(10):\n                result_custom = custom_ops.roll(x, shifts, dims)\n                torch.cuda.synchronize()\n    \n    # Print key statistics\n    print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n    \n    # Profile PyTorch implementation  \n    print(\"\\n--- Profiling PyTorch Roll ---\")\n    with torch.profiler.profile(\n        activities=[torch.profiler.ProfilerActivity.CPU, torch.profiler.ProfilerActivity.CUDA],\n        record_shapes=True,\n        profile_memory=True,\n        with_stack=True\n    ) as prof_pytorch:\n        with torch.profiler.record_function(\"pytorch_roll\"):\n            for _ in range(10):\n                result_pytorch = torch.roll(x, shifts, dims)\n                torch.cuda.synchronize()\n    \n    print(prof_pytorch.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n    \n    # Memory usage comparison\n    print(\"\\n=== Memory Analysis ===\")\n    custom_events = prof.key_averages()\n    pytorch_events = prof_pytorch.key_averages()\n    \n    print(\"Custom implementation memory usage:\")\n    for event in custom_events:\n        if 'roll' in event.key.lower():\n            # Use cpu_memory_usage and cuda_memory_usage if available\n            cpu_mem = getattr(event, 'cpu_memory_usage', 0)\n            cuda_mem = getattr(event, 'cuda_memory_usage', 0)\n            if hasattr(event, 'cpu_memory_usage') and event.cpu_memory_usage:\n                print(f\"  {event.key}: CPU={cpu_mem/1024/1024:.2f} MB\")\n            if hasattr(event, 'cuda_memory_usage') and event.cuda_memory_usage:\n                print(f\"  {event.key}: CUDA={cuda_mem/1024/1024:.2f} MB\")\n    \n    print(\"\\nPyTorch implementation memory usage:\")        \n    for event in pytorch_events:\n        if 'roll' in event.key.lower():\n            cpu_mem = getattr(event, 'cpu_memory_usage', 0)\n            cuda_mem = getattr(event, 'cuda_memory_usage', 0)\n            if hasattr(event, 'cpu_memory_usage') and event.cpu_memory_usage:\n                print(f\"  {event.key}: CPU={cpu_mem/1024/1024:.2f} MB\")\n            if hasattr(event, 'cuda_memory_usage') and event.cuda_memory_usage:\n                print(f\"  {event.key}: CUDA={cuda_mem/1024/1024:.2f} MB\")\n    \n    # Alternative: use tensor size for memory estimation\n    tensor_size_mb = x.numel() * 4 / (1024 * 1024)  # float32 = 4 bytes\n    print(f\"\\nTensor size: {tensor_size_mb:.2f} MB\")\n    print(\"Note: Roll operations typically require 2x tensor size (input + output)\")\n\nprofile_roll_operations()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# CUDA Events for Detailed Timing\ndef detailed_cuda_timing():\n    \"\"\"Use CUDA events for precise kernel timing\"\"\"\n    \n    if not torch.cuda.is_available():\n        print(\"CUDA not available\")\n        return\n        \n    print(\"=== CUDA Events Timing Analysis ===\")\n    \n    # Test configurations\n    sizes = [(500, 500), (1000, 1000), (2000, 2000)]\n    \n    for size in sizes:\n        print(f\"\\n--- Tensor Size: {size} ---\")\n        \n        x = torch.randn(size, device='cuda', dtype=torch.float32)\n        shifts = [50, 100]\n        dims = [0, 1]\n        \n        # Warmup\n        for _ in range(5):\n            _ = custom_ops.roll(x, shifts, dims)\n            _ = torch.roll(x, shifts, dims)\n        torch.cuda.synchronize()\n        \n        # Custom implementation timing\n        start_event = torch.cuda.Event(enable_timing=True)\n        end_event = torch.cuda.Event(enable_timing=True)\n        \n        start_event.record()\n        for _ in range(20):\n            result_custom = custom_ops.roll(x, shifts, dims)\n        end_event.record()\n        torch.cuda.synchronize()\n        \n        custom_time = start_event.elapsed_time(end_event) / 20  # Average per operation\n        \n        # PyTorch implementation timing  \n        start_event.record()\n        for _ in range(20):\n            result_pytorch = torch.roll(x, shifts, dims)\n        end_event.record()\n        torch.cuda.synchronize()\n        \n        pytorch_time = start_event.elapsed_time(end_event) / 20  # Average per operation\n        \n        # Calculate memory bandwidth\n        bytes_per_element = 4  # float32\n        total_elements = x.numel()\n        bytes_transferred = total_elements * bytes_per_element * 2  # Read + Write\n        \n        custom_bandwidth = (bytes_transferred / 1e9) / (custom_time / 1000)  # GB/s\n        pytorch_bandwidth = (bytes_transferred / 1e9) / (pytorch_time / 1000)  # GB/s\n        \n        print(f\"Custom:   {custom_time:.3f}ms, {custom_bandwidth:.1f} GB/s\")\n        print(f\"PyTorch:  {pytorch_time:.3f}ms, {pytorch_bandwidth:.1f} GB/s\")\n        print(f\"Speedup:  {pytorch_time/custom_time:.2f}x\")\n        \n        # Verify correctness\n        assert torch.allclose(result_custom, result_pytorch), f\"Results don't match for {size}\"\n\ndetailed_cuda_timing()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# nvprof Integration (if available)\ndef nvprof_analysis():\n    \"\"\"Try to use nvprof for detailed kernel analysis\"\"\"\n    \n    print(\"=== nvprof Analysis (if available) ===\")\n    \n    if not torch.cuda.is_available():\n        print(\"CUDA not available\")\n        return\n    \n    # Create a simple test script that we can profile\n    test_script = '''\nimport torch\nimport custom_ops\n\n# Simple test\nx = torch.randn(1000, 1000, device='cuda', dtype=torch.float32)\nshifts = [100, 200]\ndims = [0, 1]\n\n# Run operations\nfor _ in range(5):\n    result = custom_ops.roll(x, shifts, dims)\n    torch.cuda.synchronize()\n    '''\n    \n    # Write test script\n    with open('/tmp/profile_test.py', 'w') as f:\n        f.write(test_script)\n    \n    # Try to run nvprof\n    try:\n        import subprocess\n        result = subprocess.run([\n            'nvprof', \n            '--print-gpu-trace',\n            '--metrics', 'achieved_occupancy,gld_efficiency,gst_efficiency',\n            'python', '/tmp/profile_test.py'\n        ], capture_output=True, text=True, timeout=30)\n        \n        if result.returncode == 0:\n            print(\"nvprof output:\")\n            print(result.stdout)\n            if result.stderr:\n                print(\"nvprof stderr:\")\n                print(result.stderr)\n        else:\n            print(\"nvprof failed or not available\")\n            print(f\"Return code: {result.returncode}\")\n            print(f\"Error: {result.stderr}\")\n            \n    except Exception as e:\n        print(f\"Could not run nvprof: {e}\")\n        print(\"This is normal in Colab - nvprof is often not available\")\n        print(\"Use the PyTorch profiler and CUDA events instead\")\n\n# Occupancy Analysis  \ndef analyze_occupancy():\n    \"\"\"Analyze theoretical occupancy of our kernel\"\"\"\n    \n    print(\"=== Occupancy Analysis ===\")\n    \n    # Get GPU properties\n    if torch.cuda.is_available():\n        device = torch.cuda.current_device()\n        props = torch.cuda.get_device_properties(device)\n        \n        print(f\"GPU: {props.name}\")\n        print(f\"Compute Capability: {props.major}.{props.minor}\")\n        print(f\"Multiprocessors: {props.multi_processor_count}\")\n        print(f\"Max threads per MP: {props.max_threads_per_multiprocessor}\")\n        print(f\"Max blocks per MP: {props.max_blocks_per_multiprocessor}\")\n        print(f\"Warp size: {props.warp_size}\")\n        print(f\"Total memory: {props.total_memory / 1024**3:.1f} GB\")\n        \n        # These attributes might not be available in all PyTorch versions\n        if hasattr(props, 'max_threads_per_block'):\n            print(f\"Max threads per block: {props.max_threads_per_block}\")\n        if hasattr(props, 'max_shared_memory_per_block'):\n            print(f\"Shared memory per block: {props.max_shared_memory_per_block / 1024:.1f} KB\")\n        if hasattr(props, 'max_registers_per_block'):\n            print(f\"Registers per block: {props.max_registers_per_block}\")\n        \n        # Analyze our kernel configuration\n        threads_per_block = 256  # Our kernel uses 256 threads\n        \n        # Calculate theoretical occupancy based on threads\n        warps_per_block = threads_per_block // props.warp_size\n        max_warps_per_mp = props.max_threads_per_multiprocessor // props.warp_size\n        \n        # Maximum blocks limited by threads\n        max_blocks_by_threads = props.max_threads_per_multiprocessor // threads_per_block\n        \n        # Use the smaller of the two limits\n        max_active_blocks = min(max_blocks_by_threads, props.max_blocks_per_multiprocessor)\n        \n        # Calculate occupancy\n        active_warps = max_active_blocks * warps_per_block\n        occupancy_percentage = (active_warps / max_warps_per_mp) * 100\n        \n        print(f\"\\n=== Kernel Configuration Analysis ===\")\n        print(f\"Threads per block: {threads_per_block}\")\n        print(f\"Warps per block: {warps_per_block}\")\n        print(f\"Max active blocks per MP: {max_active_blocks}\")\n        print(f\"Active warps per MP: {active_warps}\")\n        print(f\"Max warps per MP: {max_warps_per_mp}\")\n        print(f\"Theoretical occupancy: {occupancy_percentage:.1f}%\")\n        \n        # Memory bandwidth estimation\n        memory_clock = getattr(props, 'memory_clock_rate', None)  # in kHz\n        memory_bus_width = getattr(props, 'memory_bus_width', None)  # in bits\n        \n        if memory_clock and memory_bus_width:\n            # Convert to GB/s: (clock_rate * bus_width * 2) / 8 / 10^6\n            theoretical_bandwidth = (memory_clock * memory_bus_width * 2) / 8 / 1e6\n            print(f\"\\n=== Memory Bandwidth ===\")\n            print(f\"Memory clock: {memory_clock/1000:.0f} MHz\")\n            print(f\"Memory bus width: {memory_bus_width} bits\")\n            print(f\"Theoretical bandwidth: {theoretical_bandwidth:.1f} GB/s\")\n        \n        # Performance recommendations\n        print(f\"\\n=== Performance Analysis ===\")\n        if occupancy_percentage < 50:\n            print(\"⚠️  Low occupancy - consider optimizing:\")\n            print(\"   • Reduce register usage per thread\")\n            print(\"   • Adjust block size (try 128 or 512 threads)\")\n            print(\"   • Check shared memory usage\")\n        elif occupancy_percentage < 75:\n            print(\"✅ Reasonable occupancy\")\n            print(\"   • Performance is likely memory-bound\")\n            print(\"   • Focus on memory access patterns\")\n        else:\n            print(\"🚀 High occupancy achieved!\")\n            print(\"   • Good thread utilization\")\n            print(\"   • Optimize memory access patterns for further gains\")\n            \n        # Note about nvprof\n        if props.major >= 7 and props.minor >= 5:\n            print(f\"\\n📝 Note: GPU has compute capability {props.major}.{props.minor}\")\n            print(\"   nvprof is not supported for CC 7.5+\")\n            print(\"   Use Nsight Compute (ncu) or Nsight Systems (nsys) instead\")\n            \n    else:\n        print(\"CUDA not available for occupancy analysis\")\n\n# Run analyses\nnvprof_analysis()\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\nanalyze_occupancy()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Memory Bandwidth Analysis",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def analyze_memory_patterns():\n    \"\"\"Analyze memory access patterns and bandwidth utilization\"\"\"\n    \n    if not torch.cuda.is_available():\n        print(\"CUDA not available\")\n        return\n        \n    print(\"=== Memory Bandwidth Analysis ===\")\n    \n    # Get theoretical peak bandwidth\n    device = torch.cuda.current_device() \n    props = torch.cuda.get_device_properties(device)\n    \n    # Estimate peak bandwidth (this varies by GPU model)\n    # Common values: GTX 1080 Ti: ~484 GB/s, RTX 2080: ~448 GB/s, Tesla V100: ~900 GB/s\n    gpu_name = props.name.lower()\n    if 'v100' in gpu_name:\n        peak_bandwidth = 900  # GB/s\n    elif 'a100' in gpu_name:\n        peak_bandwidth = 1555  # GB/s\n    elif 'rtx' in gpu_name or 'geforce' in gpu_name:\n        peak_bandwidth = 400  # GB/s (conservative estimate)\n    elif 'tesla' in gpu_name:\n        peak_bandwidth = 500  # GB/s (conservative estimate)\n    else:\n        peak_bandwidth = 300  # GB/s (very conservative for unknown GPUs)\n    \n    print(f\"GPU: {props.name}\")\n    print(f\"Estimated Peak Bandwidth: {peak_bandwidth} GB/s\")\n    \n    # Test different tensor sizes to analyze scaling\n    sizes = [\n        (100, 100),      # Small: 40 KB\n        (500, 500),      # Medium: 1 MB  \n        (1000, 1000),    # Large: 4 MB\n        (2000, 2000),    # XL: 16 MB\n        (4000, 4000),    # XXL: 64 MB\n    ]\n    \n    print(f\"\\n{'Size':<12} {'Elements':<10} {'Memory':<8} {'Custom (ms)':<12} {'PyTorch (ms)':<13} {'Custom BW':<10} {'PyTorch BW':<11} {'Efficiency':<10}\")\n    print(\"-\" * 85)\n    \n    for size in sizes:\n        try:\n            elements = size[0] * size[1]\n            memory_mb = elements * 4 / (1024 * 1024)  # float32 = 4 bytes\n            \n            x = torch.randn(size, device='cuda', dtype=torch.float32)\n            shifts = [size[0]//10, size[1]//10]  # 10% shifts\n            dims = [0, 1]\n            \n            # Warmup\n            for _ in range(3):\n                _ = custom_ops.roll(x, shifts, dims)\n                _ = torch.roll(x, shifts, dims)\n            torch.cuda.synchronize()\n            \n            # Custom timing\n            start = torch.cuda.Event(enable_timing=True)\n            end = torch.cuda.Event(enable_timing=True)\n            \n            start.record()\n            for _ in range(10):\n                result_custom = custom_ops.roll(x, shifts, dims)\n            end.record()\n            torch.cuda.synchronize()\n            custom_time = start.elapsed_time(end) / 10\n            \n            # PyTorch timing\n            start.record()\n            for _ in range(10):\n                result_pytorch = torch.roll(x, shifts, dims)\n            end.record()\n            torch.cuda.synchronize()\n            pytorch_time = start.elapsed_time(end) / 10\n            \n            # Calculate bandwidth (read + write)\n            bytes_transferred = elements * 4 * 2  # 4 bytes per float, read + write\n            custom_bw = (bytes_transferred / 1e9) / (custom_time / 1000)\n            pytorch_bw = (bytes_transferred / 1e9) / (pytorch_time / 1000)\n            \n            # Efficiency relative to peak\n            custom_eff = custom_bw / peak_bandwidth * 100\n            pytorch_eff = pytorch_bw / peak_bandwidth * 100\n            \n            print(f\"{str(size):<12} {elements:<10} {memory_mb:<8.1f} {custom_time:<12.3f} {pytorch_time:<13.3f} {custom_bw:<10.1f} {pytorch_bw:<11.1f} {custom_eff:<10.1f}%\")\n            \n        except RuntimeError as e:\n            if \"out of memory\" in str(e):\n                print(f\"{str(size):<12} {'OOM':<10}\")\n                break\n            else:\n                raise e\n    \n    print(f\"\\n=== Memory Access Pattern Analysis ===\")\n    print(\"Roll operation characteristics:\")\n    print(\"✓ Read: Sequential access to input tensor\")\n    print(\"✓ Write: Scattered access to output tensor (depends on shift pattern)\")\n    print(\"✓ Memory traffic: 2x tensor size (read input + write output)\")\n    print(\"✓ No intermediate storage needed (constant memory for metadata)\")\n    \n    print(f\"\\nOptimization opportunities:\")\n    print(\"• Coalesced memory access depends on shift alignment\")\n    print(\"• Large shifts may cause poor cache locality\")\n    print(\"• Small tensors may be bandwidth-limited rather than compute-limited\")\n\ndef compare_shift_patterns():\n    \"\"\"Compare performance with different shift patterns\"\"\"\n    \n    if not torch.cuda.is_available():\n        print(\"CUDA not available\")\n        return\n        \n    print(\"=== Shift Pattern Analysis ===\")\n    \n    size = (1000, 1000)\n    x = torch.randn(size, device='cuda', dtype=torch.float32)\n    \n    # Different shift patterns\n    patterns = [\n        ([1, 1], [0, 1], \"Small shifts (cache-friendly)\"),\n        ([100, 100], [0, 1], \"Medium shifts\"),\n        ([500, 500], [0, 1], \"Large shifts (worst case)\"),\n        ([0, 100], [0, 1], \"Single dimension shift\"),\n        ([13, 17], [0, 1], \"Prime number shifts\"),\n    ]\n    \n    print(f\"{'Pattern':<30} {'Custom (ms)':<12} {'PyTorch (ms)':<13} {'Speedup':<8}\")\n    print(\"-\" * 65)\n    \n    for shifts, dims, description in patterns:\n        # Warmup\n        for _ in range(3):\n            _ = custom_ops.roll(x, shifts, dims)\n            _ = torch.roll(x, shifts, dims)\n        torch.cuda.synchronize()\n        \n        # Timing\n        start = torch.cuda.Event(enable_timing=True)\n        end = torch.cuda.Event(enable_timing=True)\n        \n        # Custom\n        start.record()\n        for _ in range(20):\n            result_custom = custom_ops.roll(x, shifts, dims)\n        end.record()\n        torch.cuda.synchronize()\n        custom_time = start.elapsed_time(end) / 20\n        \n        # PyTorch\n        start.record()\n        for _ in range(20):\n            result_pytorch = torch.roll(x, shifts, dims)\n        end.record()\n        torch.cuda.synchronize()\n        pytorch_time = start.elapsed_time(end) / 20\n        \n        speedup = pytorch_time / custom_time\n        print(f\"{description:<30} {custom_time:<12.3f} {pytorch_time:<13.3f} {speedup:<8.2f}x\")\n\n# Run the analyses\nanalyze_memory_patterns()\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\ncompare_shift_patterns()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Optimization Summary and Recommendations",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def optimization_summary():\n    \"\"\"Summary of profiling results and optimization recommendations\"\"\"\n    \n    print(\"=== Optimization Summary ===\")\n    print()\n    print(\"🔧 Current Implementation Features:\")\n    print(\"   ✓ Constant memory for dimension metadata (fast access)\")\n    print(\"   ✓ Direct memory access without shared memory layer\")\n    print(\"   ✓ Branch-free kernel execution\")\n    print(\"   ✓ Efficient dimension filtering (skip zero shifts)\")\n    print(\"   ✓ Support for up to 8 dimensions\")\n    print()\n    \n    print(\"📊 Key Findings from Profiling:\")\n    print(\"   • PyTorch uses tensor slicing/concatenation rather than custom kernels\")\n    print(\"   • Our custom kernel is optimized for memory bandwidth\")\n    print(\"   • Performance depends heavily on:\")\n    print(\"     - Tensor size (small tensors: overhead dominates)\")\n    print(\"     - Shift patterns (large shifts: poor cache locality)\")\n    print(\"     - Memory coalescing (depends on dimension strides)\")\n    print()\n    \n    print(\"🚀 Optimization Recommendations:\")\n    print()\n    print(\"1. **Hybrid Approach**: Consider switching to PyTorch's tensor operations\")\n    print(\"   for smaller tensors where kernel launch overhead dominates\")\n    print()\n    print(\"2. **Memory Coalescing**: For better coalescing:\")\n    print(\"   • Process contiguous dimensions first\")\n    print(\"   • Consider transpose operations for better access patterns\")\n    print()\n    print(\"3. **Kernel Optimization**:\")\n    print(\"   • Current kernel uses 256 threads/block - good for most GPUs\")\n    print(\"   • Consider dynamic block sizing based on tensor dimensions\")\n    print(\"   • Investigate vectorized loads (float4) for better bandwidth\")\n    print()\n    print(\"4. **When to Use Custom vs PyTorch**:\")\n    print(\"   • Use custom kernel for: Large tensors (>1M elements)\")\n    print(\"   • Use PyTorch's roll for: Small tensors, complex multi-dim patterns\")\n    print()\n    \n    print(\"🎯 Next Steps for Further Optimization:\")\n    print(\"   1. Implement vectorized memory access (float2/float4)\")\n    print(\"   2. Add tensor size-based dispatch (custom vs PyTorch)\")\n    print(\"   3. Optimize for specific shift patterns\")\n    print(\"   4. Consider texture memory for read-only input\")\n    print(\"   5. Implement occupancy-driven block size selection\")\n    print()\n    \n    print(\"📋 Usage Guidelines:\")\n    print(\"   • For research/prototyping: Use PyTorch's roll (reliable, well-tested)\")\n    print(\"   • For production with large tensors: Use custom kernel\")\n    print(\"   • For mixed workloads: Implement size-based dispatch\")\n    print(\"   • Always profile your specific use case!\")\n\ndef final_benchmark_summary():\n    \"\"\"Run a final comprehensive benchmark and show results\"\"\"\n    \n    if not torch.cuda.is_available():\n        print(\"CUDA not available for final benchmark\")\n        return\n        \n    print(\"=== Final Benchmark Summary ===\")\n    \n    # Representative test cases\n    test_cases = [\n        ((100, 100), \"Small tensor\"),\n        ((1000, 1000), \"Medium tensor\"), \n        ((3000, 3000), \"Large tensor\"),\n    ]\n    \n    print(f\"{'Case':<15} {'Custom (ms)':<12} {'PyTorch (ms)':<13} {'Speedup':<8} {'Recommendation':<20}\")\n    print(\"-\" * 75)\n    \n    for size, description in test_cases:\n        try:\n            x = torch.randn(size, device='cuda', dtype=torch.float32)\n            shifts = [size[0]//10, size[1]//10]\n            dims = [0, 1]\n            \n            # Warmup\n            for _ in range(3):\n                _ = custom_ops.roll(x, shifts, dims)\n                _ = torch.roll(x, shifts, dims)\n            torch.cuda.synchronize()\n            \n            # Timing\n            start = torch.cuda.Event(enable_timing=True)\n            end = torch.cuda.Event(enable_timing=True)\n            \n            # Custom\n            start.record()\n            for _ in range(10):\n                result_custom = custom_ops.roll(x, shifts, dims)\n            end.record()\n            torch.cuda.synchronize()\n            custom_time = start.elapsed_time(end) / 10\n            \n            # PyTorch\n            start.record()\n            for _ in range(10):\n                result_pytorch = torch.roll(x, shifts, dims)\n            end.record()\n            torch.cuda.synchronize()\n            pytorch_time = start.elapsed_time(end) / 10\n            \n            speedup = pytorch_time / custom_time\n            \n            if speedup > 1.2:\n                recommendation = \"✅ Use Custom\"\n            elif speedup > 0.8:\n                recommendation = \"⚖️ Either works\"\n            else:\n                recommendation = \"❌ Use PyTorch\"\n                \n            print(f\"{description:<15} {custom_time:<12.3f} {pytorch_time:<13.3f} {speedup:<8.2f}x {recommendation:<20}\")\n            \n        except RuntimeError as e:\n            if \"out of memory\" in str(e):\n                print(f\"{description:<15} {'OOM':<12}\")\n            else:\n                raise e\n    \n    print()\n    print(\"✅ Profiling complete! Use the analysis above to guide your optimization decisions.\")\n\n# Run final analysis\noptimization_summary()\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\nfinal_benchmark_summary()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}